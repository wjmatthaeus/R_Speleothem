---
title: "Coding Demo Day 2 - Following L.K. McDonough et al. 2022 Past fires and post-fire impacts reconstructed [from] stalagmite  - R Notebook"
output: github_document
editor_options:
  chunk_output_type: inline
---
Following article "Past fires and post-fire impacts reconstructed from a southwest Australian stalagmite" ~ [L.K. McDonough et al. 2022](https://doi-org.ezproxy.baylor.edu/10.1016/j.gca.2022.03.020)

A note on coding style. I'm showcasing 'high-level' scripting here, not in the quality sense, but in the development sense. For languages like R, Python and Matlab it is often more time (cpu cycles) and space (memory) efficient to use vectorized or recursive operations than loops. It is expected that the developers have worked this out. For that reason, my M.O. in coding this was to find the best existing software and commands to perform each step. As it turns out I didn't need to use any basic computer science operations like loops or conditional. For background on syntax and optimizing strategies see [the R manual](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Loops-and-conditional-execution) and [this blogpost](http://rstudio-pubs-static.s3.amazonaws.com/5526_83e42f97a07141e88b75f642dbae8b1b.html), and see the illustrative example "Benchmark - Fibonacci sequence" in this [post](https://stackoverflow.com/questions/42393658/lapply-vs-for-loop-performance-r). 

Start by loading packages and importing data (excel downloaded from [here]( https://www.sciencedirect.com/science/article/abs/pii/S0016703722001454), which is also hosted on my github.

Method 1) Using Excel, save data as .csv. Remove everything except column headers and data (*Note). Read as with previous data, using read_csv from my [previous notebook](github.com/wjmatthaeus/R_Speleothem/blob/main/hendy_v1.md)
Method 2) Showcased here. Tidyverse (also described in my previous notebook) readexcel package is new to us.

```{r}


####written by W.J. Matthaeus 2022 for Monta√±ez lab group coding tutorial

#the default working directory set by RStudio is the root directory
#(on OSX root is ~ for short, and probably C: in Windsows),
#set the working directory to the directory containing your code and input files
#for PC users this will look like "C:/path/to/files", 
#NOTE: the slashes are opposite direction
setwd("~/Dropbox/R_on_git/R_Speleothem")

#packages for:
#data input and manipulation
library(tidyverse)
library(tidymodels)
#install.packages("readxl")
library(readxl)
#EDA
# install.packages("corrplot")
library(corrplot)
#Time series analysis
# install.packages('zoo')
library(zoo)
#PCA
# install.packages("FactoMineR")
# install.packages("factoextra")
library(FactoMineR)
library(factoextra)

```


Import data and take a look. Always look at your variables to avoid headaches later.


```{r}

#import
hi_res <- read_excel(path = "1-s2.0-S0016703722001454-mmc1.xlsx",sheet =1 )
#take a look
hi_res

```


So far looks good. Now for some QC.


```{r}

#look at column names
colnames(hi_res) 
#drop 17 and 18, these are empty range in the excel table cause by the *NOTE entry
hi_res <- hi_res %>% select(-(c(17,18)))
#look at the column names
colnames(hi_res)
#i don't like the way the deltas came in, change them
hi_res<-rename(hi_res, d18O="\u03b418O (\u2030)")
hi_res<-rename(hi_res, d13C="\u03b413C (\u2030)")
#these long names will wreak havoc later, change them
hi_res<-rename(hi_res, "Br (ppm)"="Br (ppm) smoothed using 13 point Savitsky Golay filter")
hi_res<-rename(hi_res, "OM (rgc)"="Organic matter (relative greyscale concentration)")
hi_res<-rename(hi_res, "S (rgc)"="S (relative greyscale concentration)")
colnames(hi_res)
#better
              
```

Data structure (i.e., the tibble) is looking clean. Let's do some EDA (exploratory data analysis)
I'm going to rearrange the data so it is more convenient for plotting. I'll use tidyr::pivot_longer() to convert to a 'long' format, wherein each row is a single observation (i.e., data column), rather than a group of observations (several data columns) for a single timepoint.

```{r}

#you can print summar statistics for every column using
summary(hi_res)

#drop columns not used in rolling average plot, and lengthen data
#this could also be done with a loop i.e., 
hi_res_long <-  hi_res %>% select(!c("Al (ppm)", "Pb (ppm)", "Cu (ppm)","Zn (ppm)"))%>%
  pivot_longer(!c("DFT (mm)","Year (CE)"),names_to = "Type",values_to = "Observation") %>% group_by(Type)

#take a look
hi_res_long

#make the new Type variable a factor (categorical) variable, and tell R what the order should be
hi_res_long$Type <- factor(hi_res_long$Type, 
  levels = c("d13C","d18O",
 "Mg (ppm)","OM (rgc)",
  "P (ppm)","S (rgc)",                           
 "Sr (ppm)","Ba (ppm)",
  "U (ppm)","Br (ppm)"))

#histograms of all variables
ggplot(data = hi_res_long)+geom_histogram(aes(x=Observation))+facet_wrap(Type~.,scales = "free")

#d18O and d13C scatterplot. nonequillibrium processes at play?
ggplot(data = hi_res)+geom_point(aes(x=d18O, y=d13C))+theme_minimal()

#pairwise correlations?  stats::cor and corrplot::corrplot
hi_res_corr <- hi_res %>% select(!c("DFT (mm)","Year (CE)")) %>% cor()
corrplot(hi_res_corr, type = "lower", order = "hclust", 
         tl.col = "black", tl.srt = 45)

#since we're doing time series analysis, lets also look at the time variable
#this is the base R plotting function, fine for single variables
#this line should be straight if the time series is regular (just a preliminary check)
plot(hi_res$`Year (CE)`)
#for syntax reference
# plot(hi_res$`Year (CE)`, hi_res$`DFT (mm)`,xlim = rev(range(hi_res$`Year (CE)`)))
```


Now following article "Past fires and post-fire impacts reconstructed from a southwest Australian stalagmite" ~ L.K. McDonough et al. 2022. overlay raw data with 5-year moving average (100 timesteps)

I'll check the time intervals for irregularity using zoo::is.regular...they turn out to be irregular.
I'll use the rollmean function from the zoo package, which is intended for use with irregular time series to calculate the rolling mean separately for each data column.
This may not affect the outcome very much, but there's no reason not to use the best tool for the job.
It's a good idea to look into the packages you use.
(https://cran.r-project.org/web/packages/zoo/index.html, doi: 10.18637/jss.v014.i06)


```{r}

#alternatively, we could use stats from base R do define a filter to calculate moving average like this
# ma <- function(x, n = 100){stats::filter(x, base::rep(1 / n, n), sides = 2)}
#where n is the window size, rep 1/n produces an even weighting, and sides = 2 produces a centered rolling mean
#or we could use lapply
#the problem with this is that it is not robust to uneven time intervals

#directly check to see if the time series is strictly regular as proivded
is.regular(zoo(x=hi_res$d13C,order.by = hi_res$`Year (CE)`))

#three nested functions to be applied groupwise below 
#read from inside out: store as zoo datatype, calculate rolling mean, then store as numeric vector
my_rollmean <- function(x, na.rm=FALSE)(as.numeric(rollmean(zoo(x,order.by = hi_res$`Year (CE)`), k=100, fill = NA)))

#calculate group means for each non-index column, then store like '_long' above
hi_res_means <- hi_res %>% mutate_at(vars(-'DFT (mm)', -'Year (CE)'), my_rollmean)%>%  #apply custom function
  select(!c("Al (ppm)", "Pb (ppm)", "Cu (ppm)","Zn (ppm)"))%>% #drop columns that the paper skips 
  pivot_longer(!c("DFT (mm)","Year (CE)"),names_to = "Type",values_to = "Observation") %>% #make observations long by Type
  group_by(Type) #tell R that the Type column is a partition of the observations

#i can define an ordering for the observation types so they plot in the same order as the paper
hi_res_means$Type <- factor(hi_res_means$Type, 
  levels = c("d13C","d18O",
 "Mg (ppm)","OM (rgc)",
  "P (ppm)","S (rgc)",                           
 "Sr (ppm)","Ba (ppm)",
  "U (ppm)","Br (ppm)"))

#build plot
p<-ggplot()+
  #new technique here, separate but same-shaped datasets being passed to data in separate geoms
    geom_line(data = hi_res_long,aes(x=`Year (CE)`, y=Observation, color = Type))+#lines for data, sometimes backticks `` must be used for non-standard variable names, in this case using regular tics '' or quotes "" would give ggplot one string ax your x
    geom_line(data = hi_res_means,aes(x=`Year (CE)`, y=Observation),color='black', linetype='dashed')+ #lines for rollmean
  #update formatting and break out by observation type
    theme_minimal()+facet_wrap(Type~.,scales = "free",ncol = 2)+ 
    theme(legend.position = 'none')

#view plot
p
#why the warning, how would we trace back the problem?
#try changing the fill value in  my_rollmean() to -999 and replotting
#is this really a problem?

#also not so good to look at, lets save and view separately
ggsave("raw_plus_rollmean_FireSpeleothem.png", plot = p, device = "png", 
       scale = 1, height = 8.5, width = 11, units = c("in"),
       dpi = 300, limitsize = TRUE)



```

Compare this to Figure 3 from McDonough et al 2020.

Smooth transition to PCA.
First we need to create a matrix of outcomes without independent variables (x), then we can run the PCA.

(For some background, I recommend looking at [this article](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c), which explains PCA in terms of Eigendecomposition. You might also search for explanations in terms of singular value decomposition.)


The purpose of PCA is to take high-dimensional data (in this case 14 dimensions), find out what is contributing the most to the overall variation in the dataset, and plot the variation in fewer dimensions. In this case, we're going to scale the obserations using the variance (scale.unit=TRUE). This is good because we're looking at a bunch of observation types with different units (i.e., ppm & relative concentration, that are not directly comparable), with very different variances (e.g., Br and U). Try the code without scaling to see how it affects the outcome.

Some terminology:
Eigenvalues - think of each a a property of a principle component that tell you proportionally how important it is to explaining the variance of the original data (i.e., EV1 is the importance of PC1). 

Eigenvector - is the unit vector in the direction of each PC, each value of the eigenvector is the "loading" of a va


```{r}

#drop time variables leaving only the measurements (the article calls them 'independent variables' but this is arbitrary)
#we're not interested in the variance of time, but only the measurements
x<-subset(hi_res, select=-c(`DFT (mm)`,`Year (CE)`))
#perform PCA with scaling
unit<-PCA(X = x, scale.unit = TRUE,graph = FALSE)
#...without scaling
# var<-PCA(X = x, scale.unit = FALSE)

#check eigenvalues (importance of PCs)
unit$eig
#compare to eigenvectors (loadings of variables on PCs)
Eigenvectors_PC1and2<-as.data.frame(unit$svd$V[,1:2])
colnames(Eigenvectors_PC1and2)<-c("EV1","EV2")
rownames(Eigenvectors_PC1and2)<-colnames(x)
Eigenvectors_PC1and2

#biplot and cor plot are essentially the same. 
#rotated data in biplot with arrows showing correlation of variables and axes
#cor plot with just variables and unit cor circle
fviz_pca_var(unit,geom = c("point","text"))
#default biplot
fviz_pca_biplot(unit)

#plot individual datapoints projected onto PCs in 'timeXPC space' 
plot(1:length(unit$ind$contrib[,1]),unit$ind$coord[,1],type = 'l', col='blue')
plot(1:length(unit$ind$contrib[,2]),unit$ind$coord[,2],type = 'l', col='orange')
plot(1:length(unit$ind$contrib[,3]),unit$ind$coord[,3],type = 'l', col='green')





##cluster variables in pc space
#following paper
set.seed(162)
unit.kmeans<-kmeans(unit$var$coord, centers = 3, nstart = 25)
grps <- as.factor(unit.kmeans$cluster)
fviz_pca_var(unit,geom = c("point","text"),col.var = grps)

#bonus using high featured command, run this in your console without 'nb.clust' for some real fun
# unit.hcpc <- HCPC(unit, nb.clust = 3, graph = TRUE)


#... but with a whimper...


```

Compare the above to Fig 5 McDonough et al 2020.
